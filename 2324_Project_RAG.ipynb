{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tnUOq3dZeDK1"
      },
      "source": [
        "# RAG System Using Llama2 With Hugging Face and Gradio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jV_2ynbxCT5n"
      },
      "source": [
        "## ---------- Project Setup ----------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZpjynrEJMIpL"
      },
      "source": [
        "### Constants and Configuration\n",
        "\n",
        "Constants used throughout the notebook are defined here.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Bes43-EMGjj"
      },
      "outputs": [],
      "source": [
        "# Define project name\n",
        "PROJECT_NAME = \"2023_2024_Project_RAG\"\n",
        "\n",
        "# Define virtual environment directory\n",
        "ENV_DIRECTORY = \"env\"\n",
        "\n",
        "# List of necessary packages to install\n",
        "PACKAGES_TO_INSTALL = [\n",
        "    \"pypdf\",\n",
        "    \"transformers\",\n",
        "    \"einops\",\n",
        "    \"accelerate\",\n",
        "    \"langchain\",\n",
        "    \"bitsandbytes\",\n",
        "    \"sentence_transformers\",\n",
        "    \"langchain\",\n",
        "    \"llama_index\",\n",
        "    \"llama-index-llms-huggingface\",\n",
        "    \"llama-index-embeddings-langchain\",\n",
        "    \"gradio\"\n",
        "]\n",
        "\n",
        "# Define data directory\n",
        "DATA_DIRECTORY = \"data\"\n",
        "\n",
        "# Define the model name\n",
        "MODEL_NAME = \"meta-llama/Llama-2-7b-chat-hf\"\n",
        "\n",
        "# Define the tokenizer name\n",
        "TOKENIZER_NAME = \"meta-llama/Llama-2-7b-chat-hf\"\n",
        "\n",
        "# Define the embedding model name\n",
        "EMBEDDING_MODEL_NAME = \"sentence-transformers/all-mpnet-base-v2\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xDHMPzICH4eD"
      },
      "source": [
        "### Mount Google Drive\n",
        "\n",
        "This section includes functions to mount Google Drive if it's not already mounted."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FSXXk6lWVyzL"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "def mount_google_drive():\n",
        "    \"\"\"\n",
        "    Mounts Google Drive if not already mounted.\n",
        "    \"\"\"\n",
        "    if not os.path.exists('/content/drive/'):\n",
        "        drive.mount('/content/drive')\n",
        "    else:\n",
        "        print(\"Google Drive is already mounted.\")\n",
        "\n",
        "# Mount Google Drive\n",
        "mount_google_drive()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RN7HzGqLVakb"
      },
      "source": [
        "### Retrieve Project Folder Path\n",
        "\n",
        "This function retrieves the path to the project folder based on the provided project name and base folder location."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s63WbTsDVwWO"
      },
      "outputs": [],
      "source": [
        "def get_project_folder_path(project_name: str, base_folder: str = '/content/drive/MyDrive/') -> str:\n",
        "    \"\"\"\n",
        "    Get the path to the project folder.\n",
        "\n",
        "    Args:\n",
        "    - project_name (str): Name of the project folder.\n",
        "    - base_folder (str): Base folder where the project folder is located. Defaults to '/content/drive/MyDrive/'.\n",
        "\n",
        "    Returns:\n",
        "    - str: Path to the project folder.\n",
        "    \"\"\"\n",
        "    # Construct project folder path\n",
        "    project_path = os.path.join(base_folder, project_name)\n",
        "\n",
        "    # Check if the project folder exists\n",
        "    if not os.path.exists(project_path):\n",
        "        print(f\"Error: Project folder '{project_name}' does not exist.\")\n",
        "        return \"\"\n",
        "\n",
        "    print(f\"Retrieved project folder path: {project_path}\")\n",
        "    return project_path\n",
        "\n",
        "# Get the path to the project folder\n",
        "project_path = get_project_folder_path(PROJECT_NAME)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "noIN6PW_W6IW"
      },
      "source": [
        "### Setting Up Virtual Environment\n",
        "\n",
        "A virtual environment is utilized to store installed packages, speeding up future sessions by avoiding the need to reinstall everything each time a new Google Colab session is launched.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aBwCLMz-Vq4n"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "def initialize_virtual_environment(project_path: str, env_directory: str) -> str:\n",
        "    \"\"\"\n",
        "    Initialize a virtual environment for the project.\n",
        "\n",
        "    Args:\n",
        "    - project_path (str): Path to the project folder.\n",
        "    - env_directory (str): Name of the directory for the virtual environment.\n",
        "\n",
        "    Returns:\n",
        "    - str: Path to the activate script of the virtual environment.\n",
        "    \"\"\"\n",
        "    # Install virtualenv\n",
        "    !pip install virtualenv\n",
        "\n",
        "    # Define virtual environment path\n",
        "    env_path = os.path.join(project_path, env_directory)\n",
        "\n",
        "    # Create virtual environment\n",
        "    !virtualenv {env_path}\n",
        "\n",
        "    # Define packages directory within virtual environment\n",
        "    packages_path = os.path.join(env_path, \"lib\", \"python3.10\", \"site-packages\")\n",
        "\n",
        "    # Add packages directory to sys.path\n",
        "    sys.path.append(packages_path)\n",
        "\n",
        "    # Define activate script path\n",
        "    env_activate_path = os.path.join(env_path, \"bin\", \"activate\")\n",
        "\n",
        "    print(f\"Path to activate script: {env_activate_path}\")\n",
        "    return env_activate_path\n",
        "\n",
        "# Initialize the virtual environment\n",
        "env_activate_path = initialize_virtual_environment(project_path, ENV_DIRECTORY)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sbqexUoOYuYq"
      },
      "source": [
        "### Install Packages\n",
        "\n",
        "This section covers the installation of necessary packages into the virtual environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y8o16oDvVsxe"
      },
      "outputs": [],
      "source": [
        "def install_packages(env_activate_path: str, packages_to_install: list):\n",
        "    \"\"\"\n",
        "    Install packages into the virtual environment.\n",
        "\n",
        "    Args:\n",
        "    - env_activate_path (str): Path to the activate script of the virtual environment.\n",
        "    - packages_to_install (list): List of package names to install.\n",
        "    \"\"\"\n",
        "    for package in packages_to_install:\n",
        "        print(f\"---------- Installing {package} ----------\")\n",
        "        !source {env_activate_path}; pip install {package}\n",
        "        print(f\"---------- {package} installed successfully! ----------\\n\\n\")\n",
        "\n",
        "install_packages(env_activate_path, PACKAGES_TO_INSTALL)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kSvrhqWmEfJp"
      },
      "source": [
        "### Library Imports\n",
        "\n",
        "In this section, we import the necessary libraries and set up the environment for our RAG system.\n",
        "\n",
        "- **llama_index**: Facilitates building and querying vector indexes.\n",
        "    - **SimpleDirectoryReader**: Reads documents from a directory and extracts their embeddings for indexing.\n",
        "    - **ServiceContext**: Manages the context of the indexing service, including configuration and resources.\n",
        "    - **VectorStoreIndex**: Represents a vector index storing document embeddings for retrieval.\n",
        "- **llama_index.llms.huggingface**: Provides utilities for integrating Hugging Face language models with the vector index.\n",
        "    - **HuggingFaceLLM**: Represents a Hugging Face LLM capable of generating responses based on input prompts.\n",
        "- **llama_index.core.prompts**: Offers utilities for creating input prompts.\n",
        "    - **SimpleInputPrompt**: Represents a simple input prompt.\n",
        "- **llama_index.embeddings.langchain**: Supports integrating Langchain embeddings with the vector index.\n",
        "    - **LangchainEmbedding**: Represents Langchain embeddings suitable for document embeddings.\n",
        "- **langchain.embeddings.huggingface**: Supports integrating Hugging Face embeddings with the vector index.\n",
        "    - **HuggingFaceEmbeddings**: Represents Hugging Face embeddings suitable for document embeddings.\n",
        "- **gradio**: A library for creating customizable UI components for machine learning models. It will be utilized to create an interactive interface for our RAG system."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PuXeUFU4sLAO"
      },
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import pypdf\n",
        "import torch\n",
        "from google.colab import userdata\n",
        "from llama_index.core import SimpleDirectoryReader, ServiceContext, VectorStoreIndex\n",
        "from llama_index.llms.huggingface import HuggingFaceLLM\n",
        "from llama_index.core.prompts.prompts import SimpleInputPrompt\n",
        "from llama_index.embeddings.langchain import LangchainEmbedding\n",
        "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
        "from gradio import Interface\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OeVxnE4LINlW"
      },
      "source": [
        "### Load Documents\n",
        "\n",
        "This function loads documents from a directory within the project folder.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KcU-nQ_XgxWD"
      },
      "outputs": [],
      "source": [
        "def load_documents(project_path: str, data_directory: str) -> list:\n",
        "    \"\"\"\n",
        "    Load documents from a directory within the project folder.\n",
        "\n",
        "    Args:\n",
        "    - project_path (str): Path to the project folder.\n",
        "    - data_directory (str): Name of the directory containing the documents within the project folder.\n",
        "\n",
        "    Returns:\n",
        "    - list: List of loaded documents.\n",
        "    \"\"\"\n",
        "    # Construct directory path within the project folder\n",
        "    directory_path = os.path.join(project_path, data_directory)\n",
        "\n",
        "    try:\n",
        "        # Initialize SimpleDirectoryReader to load data from the directory\n",
        "        directory_reader = SimpleDirectoryReader(directory_path)\n",
        "\n",
        "        # Load data from the directory\n",
        "        documents = directory_reader.load_data()\n",
        "        print(\"Documents loaded successfully.\")\n",
        "        return documents\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: Directory '{directory_path}' does not exist.\")\n",
        "        return []\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading documents: {e}\")\n",
        "        return []\n",
        "\n",
        "# Load documents from the directory within the project folder\n",
        "documents = load_documents(project_path, DATA_DIRECTORY)\n",
        "\n",
        "# Display the loaded documents\n",
        "print(\"Loaded Documents:\")\n",
        "for doc in documents:\n",
        "    print(doc)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6btADNRZW7ce"
      },
      "source": [
        "## ---------- Initializing the Q&A Assistant ----------\n",
        "\n",
        "In this section, we establish the necessary prompts and initialize the Hugging Face LLM to create an interactive Q&A assistant.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bgjz7kWmNaQ6"
      },
      "source": [
        "### System Prompt and Query Wrapper\n",
        "\n",
        "Here, we define the system prompt and the query wrapper prompt for the Q&A assistant.\n",
        "\n",
        "- **System Prompt**: Sets the context and objective for the Q&A assistant. It instructs the assistant to answer questions as accurately as possible based on the instructions and context provided.\n",
        "\n",
        "- **Query Wrapper Prompt**: Serves as a template for formatting queries to be input into the system. It follows the default format supported by LLama2, ensuring compatibility with the underlying model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2g7M89fKg54H"
      },
      "outputs": [],
      "source": [
        "# Define system prompt for the Q&A assistant\n",
        "system_prompt = \"\"\"\n",
        "You are a Q&A assistant. Your goal is to answer questions as\n",
        "accurately as possible based on the instructions and context provided.\n",
        "\"\"\"\n",
        "\n",
        "# Define query wrapper prompt (Default format supportable by LLama2)\n",
        "query_wrapper_prompt = SimpleInputPrompt(\"<|USER|>{query_str}<|ASSISTANT|>\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AwQrThuyXJfi"
      },
      "source": [
        "### Initializing the Hugging Face LLM\n",
        "\n",
        "Initializing the HuggingFaceLLM requires providing the previous system prompt and the query wrapper prompt as inputs. Additionally, specific configurations such as the context window size, maximum number of new tokens to generate, tokenizer name, model name, and device mapping are utilized."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GhPVaU-UjhV5"
      },
      "outputs": [],
      "source": [
        "from transformers import BitsAndBytesConfig\n",
        "\n",
        "def initialize_huggingface_llm(system_prompt: str, query_wrapper_prompt: SimpleInputPrompt, model_name: str, tokenizer_name: str) -> HuggingFaceLLM:\n",
        "    \"\"\"\n",
        "    Initialize HuggingFaceLLM for language generation.\n",
        "\n",
        "    Args:\n",
        "    - system_prompt (str): Prompt provided to the model to establish the context for language generation.\n",
        "    - query_wrapper_prompt (SimpleInputPrompt): Prompt for wrapping user queries for interaction with the model.\n",
        "    - model_name (str): Name of the pretrained model used for language generation.\n",
        "    - tokenizer_name (str): Name of the tokenizer used by the model.\n",
        "\n",
        "    Returns:\n",
        "    - HuggingFaceLLM: Initialized HuggingFaceLLM model.\n",
        "    \"\"\"\n",
        "    # Check if HF_TOKEN exists in the secrets\n",
        "    try:\n",
        "        # Attempt to retrieve the HF_TOKEN secret\n",
        "        userdata.get('HF_TOKEN')\n",
        "\n",
        "        # Print a message indicating that the HF Token was found\n",
        "        print(\"HF Token was found. No need to authenticate with Hugging Face CLI.\")\n",
        "    except userdata.SecretNotFoundError:\n",
        "        # Print a message indicating that the HF Token was not found\n",
        "        print(\"HF Token was not found.\")\n",
        "\n",
        "        # Print a message indicating the authentication process\n",
        "        print(\"Authenticating with Hugging Face CLI...\")\n",
        "\n",
        "        # Authenticate Hugging Face CLI if necessary (required for models that need authentication such as Llama-2-7b-chat-hf)\n",
        "        !huggingface-cli login\n",
        "\n",
        "    # Check if CUDA is available to adjust model_kwargs accordingly\n",
        "    if torch.cuda.is_available():\n",
        "        quantization_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype=torch.bfloat16)\n",
        "        model_kwargs = {\"quantization_config\": quantization_config}\n",
        "    else:\n",
        "        model_kwargs = {}\n",
        "\n",
        "    # Initialize HuggingFaceLLM\n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "      - context_window: The maximum length of the context window for language generation.\n",
        "      - max_new_tokens: The maximum number of new tokens to generate.\n",
        "      - generate_kwargs: Additional keyword arguments for generation, such as temperature and sampling.\n",
        "      - device_map: Device to use for model inference (e.g., \"auto\" for automatic selection).\n",
        "      - model_kwargs: Additional keyword arguments for the model, such as data type and loading configuration.\n",
        "    \"\"\"\n",
        "    llm = HuggingFaceLLM(\n",
        "        context_window=4096,\n",
        "        max_new_tokens=256,\n",
        "        generate_kwargs={\"temperature\": None, \"top_p\": None, \"do_sample\": False},\n",
        "        system_prompt=system_prompt,\n",
        "        query_wrapper_prompt=query_wrapper_prompt,\n",
        "        model_name=model_name,\n",
        "        tokenizer_name=tokenizer_name,\n",
        "        device_map=\"auto\",\n",
        "        model_kwargs=model_kwargs\n",
        "    )\n",
        "\n",
        "    return llm\n",
        "\n",
        "# Initialize HuggingFaceLLM for language generation\n",
        "llm = initialize_huggingface_llm(system_prompt, query_wrapper_prompt, MODEL_NAME, TOKENIZER_NAME)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zRlnJr4PXVjr"
      },
      "source": [
        "### Example Interaction with the Assistant\n",
        "\n",
        "To demonstrate the functionality of the assistant, we interact with it by providing a query and receiving a response:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PFAOLlDoVTvs"
      },
      "outputs": [],
      "source": [
        "def ask_question(llm, query):\n",
        "    \"\"\"\n",
        "    Ask a question to the assistant and get the response.\n",
        "\n",
        "    Args:\n",
        "    - llm (HuggingFaceLLM): Initialized HuggingFaceLLM model.\n",
        "    - query (str): Question to ask the assistant.\n",
        "\n",
        "    Returns:\n",
        "    - str: Response from the assistant.\n",
        "    \"\"\"\n",
        "    # Query the assistant\n",
        "    response = llm.complete(f\"User: {query}\\nAssistant:\")\n",
        "\n",
        "    # Print the query and model's response\n",
        "    print(f\"Query:\\n{query}\\n\")\n",
        "    print(f\"Model's response:\\n{response}\\n\")\n",
        "\n",
        "    return response\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    response = ask_question(llm, \"Do you see the documents in my project folder?\")\n",
        "    response = ask_question(llm, \"Say something funny?\")\n",
        "    response = ask_question(llm, \"Do you want to fly?\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p3-5wHIcfTwA"
      },
      "source": [
        "## ---------- Setting Up the Information Retrieval System ----------\n",
        "Before we can begin utilizing the information retrieval system, we need to set up and initialize several components."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G6rv_D9bfkis"
      },
      "source": [
        "### Initializing HuggingFace Embeddings\n",
        "\n",
        "To start, we initialize the HuggingFaceEmbeddings model for embedding our documents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VD85vtj2kBNT"
      },
      "outputs": [],
      "source": [
        "def initialize_huggingface_embeddings(embedding_model_name: str) -> LangchainEmbedding:\n",
        "    \"\"\"\n",
        "    Initialize HuggingFaceEmbeddings for embedding models.\n",
        "\n",
        "    Args:\n",
        "    - embedding_model_name (str): Name of the HuggingFace embedding model to use.\n",
        "\n",
        "    Returns:\n",
        "    - LangchainEmbedding: Initialized HuggingFaceEmbeddings model.\n",
        "    \"\"\"\n",
        "    # Initialize HuggingFaceEmbeddings for embedding models\n",
        "    embed_model = LangchainEmbedding(HuggingFaceEmbeddings(model_name=embedding_model_name))\n",
        "\n",
        "    return embed_model\n",
        "\n",
        "# Initialize HuggingFaceEmbeddings for embedding models\n",
        "embed_model = initialize_huggingface_embeddings(EMBEDDING_MODEL_NAME)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xztLACZOf20f"
      },
      "source": [
        "### Creating Service Context\n",
        "\n",
        "Next, we create a service context that will be used for indexing and querying."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LduiJD2ajpy4"
      },
      "outputs": [],
      "source": [
        "def create_service_context(llm: HuggingFaceLLM, embed_model: LangchainEmbedding) -> ServiceContext:\n",
        "    \"\"\"\n",
        "    Create service context for the index.\n",
        "\n",
        "    Args:\n",
        "    - llm (HuggingFaceLLM): Initialized HuggingFaceLLM model.\n",
        "    - embed_model (LangchainEmbedding): Initialized HuggingFaceEmbeddings model.\n",
        "\n",
        "    Returns:\n",
        "    - ServiceContext: Created service context.\n",
        "    \"\"\"\n",
        "    # Create service context for the index\n",
        "    service_context = ServiceContext.from_defaults(\n",
        "        chunk_size=1024,\n",
        "        llm=llm,\n",
        "        embed_model=embed_model\n",
        "    )\n",
        "\n",
        "    return service_context\n",
        "\n",
        "# Create service context for the index\n",
        "service_context = create_service_context(llm, embed_model)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USt0z-P0f_jb"
      },
      "source": [
        "###Â Loading and Creating Index\n",
        "\n",
        "Now, we load documents from a directory and create a vector store index."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uXHL2ezeldV1"
      },
      "outputs": [],
      "source": [
        "def load_and_create_index(project_path: str, data_directory: str, service_context: ServiceContext) -> VectorStoreIndex:\n",
        "    \"\"\"\n",
        "    Load documents from a directory and create a vector store index.\n",
        "\n",
        "    Args:\n",
        "    - project_path (str): Path to the project folder.\n",
        "    - data_directory (str): Name of the directory containing the documents within the project folder.\n",
        "    - service_context (ServiceContext): Service context for the index.\n",
        "\n",
        "    Returns:\n",
        "    - VectorStoreIndex: Created vector store index.\n",
        "    \"\"\"\n",
        "    # Construct directory path within the project folder\n",
        "    directory_path = os.path.join(project_path, data_directory)\n",
        "\n",
        "    try:\n",
        "        # Load documents from the directory\n",
        "        documents = SimpleDirectoryReader(directory_path).load_data()\n",
        "\n",
        "        # Create vector store index from the documents\n",
        "        index = VectorStoreIndex.from_documents(documents, service_context=service_context)\n",
        "        return index\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: Directory '{directory_path}' does not exist.\")\n",
        "        return None\n",
        "\n",
        "# Load documents from the directory and create a vector store index\n",
        "index = load_and_create_index(project_path, DATA_DIRECTORY, service_context)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Agcm9SLgDw7"
      },
      "source": [
        "### Converting Index to Query Engine\n",
        "\n",
        "Lastly, we convert the index into a query engine, allowing us to perform queries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uSJzrMm6kTxf"
      },
      "outputs": [],
      "source": [
        "def convert_index_to_query_engine(index: VectorStoreIndex):\n",
        "    \"\"\"\n",
        "    Convert index into a query engine.\n",
        "\n",
        "    Args:\n",
        "    - index (VectorStoreIndex): Vector store index.\n",
        "\n",
        "    Returns:\n",
        "    - QueryEngine: Converted query engine.\n",
        "    \"\"\"\n",
        "    # Convert index into a query engine\n",
        "    query_engine = index.as_query_engine()\n",
        "\n",
        "    return query_engine\n",
        "\n",
        "# Convert index into a query engine\n",
        "query_engine = convert_index_to_query_engine(index)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CTfPGV4-gGL9"
      },
      "source": [
        "### Performing Queries and Printing Responses\n",
        "\n",
        "Now that our query engine is set up, we can utilize it to perform queries and print their responses."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wn_LZR7Nfbpn"
      },
      "outputs": [],
      "source": [
        "def query_model(text):\n",
        "    \"\"\"\n",
        "    Query the RAG system model.\n",
        "\n",
        "    Args:\n",
        "    - text (str): Input text from the user.\n",
        "\n",
        "    Returns:\n",
        "    - str: Response generated by the RAG system.\n",
        "    \"\"\"\n",
        "    # Use the query engine to get a response based on the input text\n",
        "    response = query_engine.query(text)\n",
        "    return response\n",
        "\n",
        "# Perform queries and print responses\n",
        "if torch.cuda.is_available():\n",
        "    response = query_model(\"Who are the authors of guide?\")\n",
        "    print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i8CJQXQLQ6Tk"
      },
      "source": [
        "## ---------- Gradio Interface ----------\n",
        "\n",
        "With everything set up, we can deploy an interface using Gradio to interact with the RAG system in a more user-friendly manner."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LNtAX5dhtx8C"
      },
      "outputs": [],
      "source": [
        "# Create the Gradio interface\n",
        "interface = Interface(\n",
        "    fn=query_model,\n",
        "    inputs=\"text\",\n",
        "    outputs=\"text\",\n",
        "    title=\"RAG System Interface\",\n",
        "    description=\"Ask a question to get a response from the RAG system.\"\n",
        ")\n",
        "\n",
        "# Launch the Gradio interface\n",
        "interface.launch(debug=True)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
